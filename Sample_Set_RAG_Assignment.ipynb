{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE42rIf7u1Kf",
    "outputId": "2b7177d6-5ecc-4be8-fcc9-58a4d2032e79"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_ollama langchain_community pdfminer-six sentence_transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJANDD9Xuei0",
    "outputId": "00d248a2-fc14-4f11-a7cf-5edcbf95f1b0"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-lDjXZrtvJ5N"
   },
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Pk9O1FtzvoPK"
   },
   "outputs": [],
   "source": [
    "def setup_vectorstore(documents):\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"/n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    doc_chunks = text_splitter.split_documents(documents)\n",
    "    vectorstore = FAISS.from_documents(doc_chunks, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BX_4KaYNvr2h"
   },
   "outputs": [],
   "source": [
    "def create_chain(vectorstore):\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1\",\n",
    "        temperature=0\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    memory = ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        output_key=\"answer\",\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"map_reduce\",\n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/_skqtny15_l93qm804rw5w5h0000gq/T/ipykernel_56480/2683243485.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/bx/_skqtny15_l93qm804rw5w5h0000gq/T/ipykernel_56480/2683243485.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"Gen_AI_Engineer_Machine_Learning_Engineer_Assignment.pdf\"\n",
    "vectorstore = setup_vectorstore(load_document(FILE_PATH))\n",
    "conversation_chain = create_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_assistant(question):\n",
    "    response = conversation_chain({\"question\": question})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9nl6w44vyT1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9oTEL0kvZEn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rixEcxj6wgiw",
    "outputId": "b47c43dd-7760-45a1-8d69-3916bc8885b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/_skqtny15_l93qm804rw5w5h0000gq/T/ipykernel_56480/1509270585.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = conversation_chain({\"question\": question})\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "Gen AI Engineer / Machine Learning Engineer Assignment\n",
      "\n",
      "Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
      "bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
      "Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
      "relevant information from a dataset and generate coherent answers.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1.\n",
      "\n",
      "Implement a RAG-based model that can handle questions related to a provided\n",
      "document or dataset.\n",
      "\n",
      "2. Use a vector database (such as Pinecone)\n",
      "\n",
      "to store and retrieve document\n",
      "\n",
      "embeddings efficiently.\n",
      "\n",
      "3. Test the model with several queries and show how well it retrieves and generates\n",
      "\n",
      "accurate answers from the document.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A Colab notebook demonstrating the entire pipeline, from data loading to question\n",
      "\n",
      "answering.\n",
      "\n",
      "● Documentation explaining the model architecture, approach to retrieval, and how\n",
      "\n",
      "generative responses are created.\n",
      "\n",
      "● Provide several example queries and the corresponding outputs.\n",
      "\n",
      "\f",
      "Part 2: Interactive QA Bot Interface\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop an interactive interface for the QA bot from Part 1, allowing users to input queries\n",
      "and retrieve answers in real time. The interface should enable users to upload documents\n",
      "and ask questions based on the content of the uploaded document.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1. Build a simple frontend interface using Streamlit or Gradio, allowing users to\n",
      "\n",
      "2.\n",
      "\n",
      "upload PDF documents and ask questions.\n",
      "Integrate the backend from Part 1 to process the PDF, store document embeddings,\n",
      "and provide real-time answers to user queries.\n",
      "\n",
      "3. Ensure that the system can handle multiple queries efficiently and provide accurate,\n",
      "\n",
      "contextually relevant responses.\n",
      "\n",
      "4. Allow users to see the retrieved document segments alongside the generated\n",
      "\n",
      "answer.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A deployed QA bot with a frontend interface where users can upload documents and\n",
      "\n",
      "interact with the bot.\n",
      "\n",
      "● Documentation on how the user can upload files, ask questions, and view the bot's\n",
      "\n",
      "responses.\n",
      "\n",
      "● Example interactions demonstrating the bot's capabilities.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "● Use Docker to containerize the application for easy deployment.\n",
      "● Ensure the system can handle large documents and multiple queries without\n",
      "\n",
      "significant performance drops.\n",
      "● Share the code, deployment\n",
      "\n",
      "GitHub.\n",
      "\n",
      "instructions, and the final working model\n",
      "\n",
      "through\n",
      "\n",
      "General Guidelines:\n",
      "\n",
      "1. Ensure modular and scalable code following best practices for both frontend and\n",
      "\n",
      "backend development.\n",
      "\n",
      "2. Document your approach thoroughly, explaining your decisions, challenges faced,\n",
      "\n",
      "and solutions.\n",
      "\n",
      "3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\n",
      "\n",
      "instructions.\n",
      "\n",
      "4. Submissions should include:\n",
      "\n",
      "○ Source code for both the notebook and the interface.\n",
      "○ A fully functional Colab notebook.\n",
      "○ Documentation on the pipeline and deployment instructions.\n",
      "Human: What are the deliverables\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "Here are the deliverables mentioned in the document:\n",
      "\n",
      "**Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot**\n",
      "\n",
      "* A Colab notebook demonstrating the entire pipeline, from data loading to question answering.\n",
      "* Documentation explaining the model architecture, approach to retrieval, and how generative responses are created.\n",
      "* Provide several example queries and the corresponding outputs.\n",
      "\n",
      "**Part 2: Interactive QA Bot Interface**\n",
      "\n",
      "* A deployed QA bot with a frontend interface where users can upload documents and interact with the bot.\n",
      "* Documentation on how the user can upload files, ask questions, and view the bot's responses.\n",
      "* Example interactions demonstrating the bot's capabilities.\n",
      "Human: What are the deliverables\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = query_assistant(\"What are the deliverables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The deliverables mentioned in the document are:\n",
      "\n",
      "1. A Colab notebook\n",
      "2. Documentation explaining the model architecture and approach to retrieval\n",
      "3. Several example queries and their corresponding outputs\n",
      "4. A deployed QA bot with a frontend interface\n",
      "5. Documentation on how users can interact with the bot\n",
      "6. Example interactions demonstrating the bot's capabilities\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What are the deliverables\n",
      "Assistant: The deliverables mentioned in the document are:\n",
      "\n",
      "1. A Colab notebook\n",
      "2. Documentation explaining the model architecture and approach to retrieval\n",
      "3. Several example queries and their corresponding outputs\n",
      "4. A deployed QA bot with a frontend interface\n",
      "5. Documentation on how users can interact with the bot\n",
      "6. Example interactions demonstrating the bot's capabilities\n",
      "Human: How to containerize the application\n",
      "Assistant: Use Docker.\n",
      "Follow Up Input: How to containerize the application\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "Gen AI Engineer / Machine Learning Engineer Assignment\n",
      "\n",
      "Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
      "bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
      "Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
      "relevant information from a dataset and generate coherent answers.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1.\n",
      "\n",
      "Implement a RAG-based model that can handle questions related to a provided\n",
      "document or dataset.\n",
      "\n",
      "2. Use a vector database (such as Pinecone)\n",
      "\n",
      "to store and retrieve document\n",
      "\n",
      "embeddings efficiently.\n",
      "\n",
      "3. Test the model with several queries and show how well it retrieves and generates\n",
      "\n",
      "accurate answers from the document.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A Colab notebook demonstrating the entire pipeline, from data loading to question\n",
      "\n",
      "answering.\n",
      "\n",
      "● Documentation explaining the model architecture, approach to retrieval, and how\n",
      "\n",
      "generative responses are created.\n",
      "\n",
      "● Provide several example queries and the corresponding outputs.\n",
      "\n",
      "\f",
      "Part 2: Interactive QA Bot Interface\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop an interactive interface for the QA bot from Part 1, allowing users to input queries\n",
      "and retrieve answers in real time. The interface should enable users to upload documents\n",
      "and ask questions based on the content of the uploaded document.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1. Build a simple frontend interface using Streamlit or Gradio, allowing users to\n",
      "\n",
      "2.\n",
      "\n",
      "upload PDF documents and ask questions.\n",
      "Integrate the backend from Part 1 to process the PDF, store document embeddings,\n",
      "and provide real-time answers to user queries.\n",
      "\n",
      "3. Ensure that the system can handle multiple queries efficiently and provide accurate,\n",
      "\n",
      "contextually relevant responses.\n",
      "\n",
      "4. Allow users to see the retrieved document segments alongside the generated\n",
      "\n",
      "answer.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A deployed QA bot with a frontend interface where users can upload documents and\n",
      "\n",
      "interact with the bot.\n",
      "\n",
      "● Documentation on how the user can upload files, ask questions, and view the bot's\n",
      "\n",
      "responses.\n",
      "\n",
      "● Example interactions demonstrating the bot's capabilities.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "● Use Docker to containerize the application for easy deployment.\n",
      "● Ensure the system can handle large documents and multiple queries without\n",
      "\n",
      "significant performance drops.\n",
      "● Share the code, deployment\n",
      "\n",
      "GitHub.\n",
      "\n",
      "instructions, and the final working model\n",
      "\n",
      "through\n",
      "\n",
      "General Guidelines:\n",
      "\n",
      "1. Ensure modular and scalable code following best practices for both frontend and\n",
      "\n",
      "backend development.\n",
      "\n",
      "2. Document your approach thoroughly, explaining your decisions, challenges faced,\n",
      "\n",
      "and solutions.\n",
      "\n",
      "3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\n",
      "\n",
      "instructions.\n",
      "\n",
      "4. Submissions should include:\n",
      "\n",
      "○ Source code for both the notebook and the interface.\n",
      "○ A fully functional Colab notebook.\n",
      "○ Documentation on the pipeline and deployment instructions.\n",
      "Human: Here is the standalone question:\n",
      "\n",
      "How do I containerize the application?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "Use Docker to containerize the application for easy deployment.\n",
      "Human: Here is the standalone question:\n",
      "\n",
      "How do I containerize the application?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = query_assistant(\"How to containerize the application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Docker.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What are the deliverables\n",
      "Assistant: The deliverables mentioned in the document are:\n",
      "\n",
      "1. A Colab notebook\n",
      "2. Documentation explaining the model architecture and approach to retrieval\n",
      "3. Several example queries and their corresponding outputs\n",
      "4. A deployed QA bot with a frontend interface\n",
      "5. Documentation on how users can interact with the bot\n",
      "6. Example interactions demonstrating the bot's capabilities\n",
      "Human: How to containerize the application\n",
      "Assistant: Use Docker.\n",
      "Human: How to containerize the application\n",
      "Assistant: Use Docker.\n",
      "Human: what should be used to build a simple frontend interface\n",
      "Assistant: I don't know. The text only mentions Streamlit or Gradio as options for uploading PDF documents and asking questions, but it doesn't provide information on their general use cases or whether they are suitable for building a simple frontend interface in general.\n",
      "Follow Up Input: What is the alternate of Streamlit\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "Gen AI Engineer / Machine Learning Engineer Assignment\n",
      "\n",
      "Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
      "bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
      "Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
      "relevant information from a dataset and generate coherent answers.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1.\n",
      "\n",
      "Implement a RAG-based model that can handle questions related to a provided\n",
      "document or dataset.\n",
      "\n",
      "2. Use a vector database (such as Pinecone)\n",
      "\n",
      "to store and retrieve document\n",
      "\n",
      "embeddings efficiently.\n",
      "\n",
      "3. Test the model with several queries and show how well it retrieves and generates\n",
      "\n",
      "accurate answers from the document.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A Colab notebook demonstrating the entire pipeline, from data loading to question\n",
      "\n",
      "answering.\n",
      "\n",
      "● Documentation explaining the model architecture, approach to retrieval, and how\n",
      "\n",
      "generative responses are created.\n",
      "\n",
      "● Provide several example queries and the corresponding outputs.\n",
      "\n",
      "\f",
      "Part 2: Interactive QA Bot Interface\n",
      "\n",
      "Problem Statement:\n",
      "\n",
      "Develop an interactive interface for the QA bot from Part 1, allowing users to input queries\n",
      "and retrieve answers in real time. The interface should enable users to upload documents\n",
      "and ask questions based on the content of the uploaded document.\n",
      "\n",
      "Task Requirements:\n",
      "\n",
      "1. Build a simple frontend interface using Streamlit or Gradio, allowing users to\n",
      "\n",
      "2.\n",
      "\n",
      "upload PDF documents and ask questions.\n",
      "Integrate the backend from Part 1 to process the PDF, store document embeddings,\n",
      "and provide real-time answers to user queries.\n",
      "\n",
      "3. Ensure that the system can handle multiple queries efficiently and provide accurate,\n",
      "\n",
      "contextually relevant responses.\n",
      "\n",
      "4. Allow users to see the retrieved document segments alongside the generated\n",
      "\n",
      "answer.\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "● A deployed QA bot with a frontend interface where users can upload documents and\n",
      "\n",
      "interact with the bot.\n",
      "\n",
      "● Documentation on how the user can upload files, ask questions, and view the bot's\n",
      "\n",
      "responses.\n",
      "\n",
      "● Example interactions demonstrating the bot's capabilities.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "● Use Docker to containerize the application for easy deployment.\n",
      "● Ensure the system can handle large documents and multiple queries without\n",
      "\n",
      "significant performance drops.\n",
      "● Share the code, deployment\n",
      "\n",
      "GitHub.\n",
      "\n",
      "instructions, and the final working model\n",
      "\n",
      "through\n",
      "\n",
      "General Guidelines:\n",
      "\n",
      "1. Ensure modular and scalable code following best practices for both frontend and\n",
      "\n",
      "backend development.\n",
      "\n",
      "2. Document your approach thoroughly, explaining your decisions, challenges faced,\n",
      "\n",
      "and solutions.\n",
      "\n",
      "3. Provide a detailed ReadMe file in your GitHub repository, including setup and usage\n",
      "\n",
      "instructions.\n",
      "\n",
      "4. Submissions should include:\n",
      "\n",
      "○ Source code for both the notebook and the interface.\n",
      "○ A fully functional Colab notebook.\n",
      "○ Documentation on the pipeline and deployment instructions.\n",
      "Human: What is an alternative to Streamlit?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "According to the text, an alternative to Streamlit is Gradio.\n",
      "Human: What is an alternative to Streamlit?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = query_assistant(\"What is the alternate of Streamlit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An alternative to Streamlit is Gradio.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
